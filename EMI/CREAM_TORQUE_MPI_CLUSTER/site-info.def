# YAIM example site configuration file - adapt it to your site!

##############################
# CE configuration variables #
##############################

CE_HOST=test06.egi.cesga.es
WMS_HOST=test22.egi.cesga.es
BDII_HOST=topbdii02.ncg.ingrid.pt
SITE_BDII_HOST=sbdii02.ncg.ingrid.pt 
##LFC_HOST=lxb7607.cern.ch
PX_HOST=myproxy.egi.cesga.es
MON_HOST=test07.egi.cesga.es 

YAIM_LOGGING_LEVEL=DEBUG


#####################################
# CREAM CE node cluster mode  #
#####################################
CREAMCE_CLUSTER_MODE=yes


# Hostname where the cluster is configured
CLUSTER_HOST="test06.egi.cesga.es"
COMPUTING_SERVICE_ID="test06.egi.cesga.es_ComputingElement" 

# Space separated list of your cluster names
CLUSTERS="clustersa2"

# Cluster UniqueID
CLUSTER_CLUSTERSA2_CLUSTER_UniqueID=my-sa2
CLUSTER_TEST06_EGI_CESGA_ES_CLUSTER_UniqueID=my-sa2

##QUEUE_xxx_CLUSTER_UniqueID 

# Cluster human readable name
CLUSTER_CLUSTERSA2_CLUSTER_Name="This is the SA2 cluster for UMD verification"
CLUSTER_TEST06_EGI_CESGA_ES_CLUSTER_Name="This is the SA2 cluster for UMD verification"


# Site name where the cluster belongs to
# It should be consistent with your variable SITE_NAME
# NOTE: This may be changed to SITE_UniqueID when the GlueSite
# is configured with the new infosys variables
CLUSTER_CLUSTERSA2_SITE_UniqueID=cesga-egi
CLUSTER_TEST06_EGI_CESGA_ES_SITE_UniqueID=cesga-egi

# space separated list of CE hostnames configured in the cluster
CLUSTER_CLUSTERSA2_CE_HOSTS="test06.egi.cesga.es"
CLUSTER_TEST06_EGI_CESGA_ES_CE_HOSTS="test06.egi.cesga.es"
CLUSTER_YAIM_CE_TYPE="cream"
CLUSTER_CLUSTERSA2__INFO_PORT=2170
CLUSTER_CLUSTERSA2__INFO_TYPE=resource

# Define the following variables for the CEs configured in your cluster
# NOTE: you don't need to uncomment these variables if you are configuring
# only one CE in the same host as your cluster.
# You can use the variables defined in services/lcg-ce instead
# CE type: 'jobmanager' for lcg CE and 'cream' for cream CE
CE_HOST_test06_egi_cesga_es_CE_TYPE="cream"

# Space separated list of the queue names configured in the CE
# This variable has been renamed in the new infosys configuration.
# The old variable name was: QUEUES
#CE_HOST_test06_egi_cesga_es_QUEUES="GRID_ops GRID_dteam GRID_opsibeu GRID_iberibeu"

# The name of the job manager used by the gatekeeper
# This variable has been renamed in the new infosys configuration.
# The old variable name was: JOB_MANAGER
# Please, define: lcgpbs, lcglfs, lcgsge or lcgcondor
CE_HOST_test06_egi_cesga_es_CE_InfoJobManager="pbs"

CE_HOST_test06_egi_cesga_es_QUEUES="GRID_ops GRID_dteam GRID_opsibeu GRID_iberibeu"
QUEUE_GRID_OPS_CLUSTER_UniqueID=my-sa2
QUEUE_GRID_DTEAM_CLUSTER_UniqueID=my-sa2
QUEUE_GRID_OPSIBEU_CLUSTER_UniqueID=my-sa2
QUEUE_GRID_IBERIBEU_CLUSTER_UniqueID=my-sa2



# The Subcluster variables should contain the name of the subcluster variable in upper case
SUBCLUSTER_TEST06_EGI_CESGA_ES_SUBCLUSTER_UniqueID=cesga-egi-configuration
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_ApplicationSoftwareRunTimeEnvironment="LCG-2|LCG-2_1_0|LCG-2_1_1|LCG-2_2_0"   # CE_RUNTIMEENV
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_ArchitectureSMPSize=2                                                         # CE_SMPSIZE
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_ArchitecturePlatformType=x86_64                                              # CE_OS_ARCH
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_BenchmarkSF00=1714                                                            # CE_SF00
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_BenchmarkSI00=2395                                                            # CE_SI00
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_MainMemoryRAMSize=513                                                         # CE_MINPHYSMEM
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_MainMemoryVirtualSize=524                                                    # CE_MINVIRTMEM
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_NetworkAdapterInboundIP=FALSE                                                 # CE_INBOUNDIP
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_NetworkAdapterOutboundIP=TRUE                                                 # CE_OUTBOUNDIP
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_OperatingSystemName="Scientific Linux"                                        # CE_OS
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_OperatingSystemRelease=5.5                                                  # CE_OS_RELEASE
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_OperatingSystemVersion="ScientificSL"                                                   # CE_OS_VERSION
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_ProcessorClockSpeed=2200                                                      # CE_CPU_SPEED
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_ProcessorModel=Opteron                                                          # CE_CPU_MODEL
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_ProcessorVendor=amd                                                        # CE_CPU_VENDOR
SUBCLUSTER_TEST06_EGI_CESGA_ES_HOST_ProcessorOtherDescription="Cores=24,Benchmark=9.58-HEP-SPEC06"             # Processor description
SUBCLUSTER_TEST06_EGI_CESGA_ES_SUBCLUSTER_Name="my subcluster SA2 verification"
SUBCLUSTER_TEST06_EGI_CESGA_ES_SUBCLUSTER_PhysicalCPUs=2                                                          # CE_PHYSCPU
SUBCLUSTER_TEST06_EGI_CESGA_ES_SUBCLUSTER_LogicalCPUs=2                                                          # CE_LOGCPU
SUBCLUSTER_TEST06_EGI_CESGA_ES_SUBCLUSTER_TmpDir=/tmp
SUBCLUSTER_TEST06_EGI_CESGA_ES_SUBCLUSTER_WNTmpDir=/tmp


################################
# Site configuration variables #
################################

SITE_EMAIL=egee-admin@cesga.es


SITE_NAME=CESGA-EGEE
SITE_LOC="Santiago de Compostela, Spain"
SITE_LAT=42.875558          #42.8757 # -90 to 90 degrees
SITE_LONG=-8.553147         #-8.5536 # -180 to 180 degrees


########################################
# Batch server configuration variables #
########################################

# Jobmanager specific settings
JOB_MANAGER=lcgpbs
BATCH_SERVER=test06.egi.cesga.es
CE_BATCH_SYS=torque
BATCH_LOG_DIR=/var/torque
BATCH_VERSION=torque-2.5.7-7

################################
# APEL configuration variables #
################################

# Database password for the APEL DB.
APEL_MYSQL_HOST=test07.egi.cesga.es
APEL_DB_PASSWORD="set_a_good_password"

#########################################

# ARGUS authorisation framework control #

#########################################



# Set USE_ARGUS to yes to enable the configuration of ARGUS

###USE_ARGUS=yes
USE_ARGUS=no


# In case ARGUS is to be used the following should be set
# The ARGUS service PEPD endpoints as a space separated list:

###ARGUS_PEPD_ENDPOINTS="https://test10.egi.cesga.es:8154/authz"
###CREAM_PEPC_RESOURCEID="http://www.egee.cesga.es/test10"


# These variables tell YAIM where to find additional configuration files.
WN_LIST=/opt/glite/yaim/etc/wn-list.conf
USERS_CONF=/opt/glite/yaim/etc/users.conf
GROUPS_CONF=/opt/glite/yaim/etc/groups.conf
FUNCTIONS_DIR=/opt/glite/yaim/functions


#
# SE_dpm-specific settings - Ignore if you are not running a DPM
#
# Set these if you are installing a DPM yourself
# and/or if you need a default DPM for the lcg-stdout-mon
#
# DPMDATA is now deprecated. Use an entry like $DPM_HOST:/filesystem in
# the DPM_FILESYSTEMS variable.
# From now on we use DPM_DB_USER and DPM_DB_PASSWORD to make clear
# its different role from that of the dpmmgr unix user who owns the
# directories and runs the daemons.


# The name of the DPM head node
DPM_HOST=test08.egi.cesga.es   

DPMPOOL=egi-pool
DPM_FILESYSTEMS="$DPM_HOST:/storage"

# The base user
DPM_DB_USER=dpmmgr
DPM_DB_HOST=$DPM_HOST
DPM_DB_PASSWORD=set_a_good_password


# Specifies the default amount of space reserved  for a file
#DPMFSIZE=200M

DPM_INFO_USER=dpm_info
DPM_INFO_PASS=set_a_good_password

# Variable for the port range  - Optional, default value is shown
# RFIO_PORT_RANGE="20000 25000"


# This largely replaces CE_CLOSE_SE but it is a list of hostnames
SE_MOUNT_INFO_LIST="none"
SE_LIST="$DPM_HOST"
SE_ARCH="multidisk" # "disk, tape, multidisk, other"



############################
# SubCluster configuration #
############################
# Architecture and enviroment specific settings
CE_CPU_MODEL=Opteron
CE_CPU_VENDOR=amd
CE_CPU_SPEED=2200

CE_OS="ScientificSL"    # Forma correcta
CE_OS_RELEASE=5.5
CE_OS_VERSION="Boron"


#New variables
CE_PHYSCPU=2
CE_LOGCPU=2
CE_OS_ARCH=x86_64
CE_CAPABILITY="CPUScalingReferenceSI00=2395"
CE_OTHERDESCR="Cores=24,Benchmark=9.58-HEP-SPEC06"
SE_MOUNT_INFO_LIST="none"
CE_SI00=2395

CE_MINPHYSMEM=524
CE_MINVIRTMEM=512
CE_SMPSIZE=2
CE_SF00=1714
CE_OUTBOUNDIP=TRUE
CE_INBOUNDIP=FALSE
CE_RUNTIMEENV="
    LCG-2
    LCG-2_1_0
    LCG-2_1_1
    LCG-2_2_0
    LCG-2_3_0
    LCG-2_3_1
    LCG-2_4_0
    LCG-2_5_0
    LCG-2_6_0
    LCG-2_7_0
    GLITE-3_0_0
    GLITE-3_0_2
    GLITE-3_1_0
    R-GMA
"

###CREAM CE Variables
CEMON_HOST=test06.egi.cesga.es
CREAM_DB_USER=umdtest
CREAM_DB_PASSWORD="set_a_good_password"
MYSQL_PASSWORD="set_a_good_password"
BLPARSER_HOST=test06.egi.cesga.es


# MPI CONFIGURATION
##################################
MPI_OPENMPI_ENABLE="yes"
MPI_OPENMPI_VERSION="1.4-4"
##If you do NOT provide a shared home, set $MPI_SHARED_HOME to "no" (default).
###MPI_SHARED_HOME="no"
## If you do NOT have SSH Hostbased Authentication between your WNs, set the below variable to "no" (default). Else, set it to "yes".
###MPI_SSH_HOST_BASED_AUTH="yes"
### If you use Torque as batch system, you may want to let the yaim plugin configure a submit filter for you. Uncomment the following line to do so
MPI_SUBMIT_FILTER="yes"

# VOS="atlas alice lhcb cms dteam biomed"
# Space separated list of supported VOs by your site
VOS="ops dteam ops.vo.ibergrid.eu iber.vo.ibergrid.eu"
QUEUES="GRID_ops GRID_dteam GRID_opsibeu GRID_iberibeu"
VO_SW_DIR=/opt/exp_soft

#New in Yaim 3.0.1
GRID_OPS_GROUP_ENABLE="ops /VO=ops/GROUP=/ops/ROLE=lcgadmin"
GRID_DTEAM_GROUP_ENABLE="dteam /VO=dteam/GROUP=/dteam/ROLE=lcgadmin"
GRID_OPSIBEU_GROUP_ENABLE="ops.vo.ibergrid.eu /VO=ops.vo.ibergrid.eu/GROUP=/ops.vo.ibergrid.eu/ROLE=VO-Admin /VO=ops.vo.ibergrid.eu/GROUP=/ops.vo.ibergrid.eu/ROLE=Production"
GRID_IBERIBEU_GROUP_ENABLE="iber.vo.ibergrid.eu /VO=iber.vo.ibergrid.eu/GROUP=/iber.vo.ibergrid.eu/ROLE=VO-Admin /VO=iber.vo.ibergrid.eu/GROUP=/iber.vo.ibergrid.eu/ROLE=Production"


#::::::::::::::
#ops
#::::::::::::::
VO_OPS_SW_DIR=$VO_SW_DIR/ops
VO_OPS_DEFAULT_SE=$DPM_HOST
VO_OPS_STORAGE_DIR=$CLASSIC_STORAGE_DIR/ops
VO_OPS_QUEUES="GRIDEGI_ops"
VO_OPS_VOMS_SERVERS="'vomss://voms.cern.ch:8443/voms/ops?/ops/'"
VO_OPS_VOMSES="'ops voms.cern.ch 15009 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch ops' 'ops lcg-voms.cern.ch 15009 /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch ops'"
VO_OPS_VOMS_CA_DN="'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority'"


#::::::::::::::
#dteam
#::::::::::::::
VO_DTEAM_SW_DIR=$VO_SW_DIR/dteam
VO_DTEAM_DEFAULT_SE=$DPM_HOST
VO_DTEAM_STORAGE_DIR=$CLASSIC_STORAGE_DIR/dteam
VO_DTEAM_QUEUES="GRID_dteam"
VO_DTEAM_VOMS_SERVERS='vomss://voms.hellasgrid.gr:8443/voms/dteam?/dteam/'
VO_DTEAM_VOMSES="'dteam lcg-voms.cern.ch 15004 /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch dteam 24' 'dteam voms.cern.ch 15004 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch dteam 24' 'dteam voms.hellasgrid.gr 15004 /C=GR/O=HellasGrid/
OU=hellasgrid.gr/CN=voms.hellasgrid.gr dteam 24' 'dteam voms2.hellasgrid.gr 15004 /C=GR/O=HellasGrid/OU=hellasgrid.gr/CN=voms2.hellasgrid.gr dteam 24'"
VO_DTEAM_VOMS_CA_DN="'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/C=GR/O=HellasGrid/OU=Certification Authorities/CN=HellasGrid CA 2006' '/C=GR/O=HellasGrid/OU=Certif
ication Authorities/CN=HellasGrid CA 2006'"

## IBERGRID VOS ##
# ops.vo.ibergrid.eu
VO_OPS_VO_IBERGRID_EU_SW_DIR=$VO_SW_DIR/opsibeu
VO_OPS_VO_IBERGRID_EU_DEFAULT_SE=$DPM_HOST
VO_OPS_VO_IBERGRID_EU_STORAGE_DIR=$CLASSIC_STORAGE_DIR/opsibeu
VO_OPS_VO_IBERGRID_EU_QUEUES="GRID_opsibeu"
VO_OPS_VO_IBERGRID_EU_VOMS_SERVERS="'vomss://voms02.ncg.ingrid.pt:8443/voms/ops.vo.ibergrid.eu?/ops.vo.ibergrid.eu'"
VO_OPS_VO_IBERGRID_EU_VOMSES="'ops.vo.ibergrid.eu voms02.ncg.ingrid.pt 40001 /C=PT/O=LIPCA/O=LIP/OU=Lisboa/CN=voms02.ncg.ingrid.pt ops.vo.ibergrid.eu'"
VO_OPS_VO_IBERGRID_EU_VOMS_CA_DN="'/C=PT/O=LIPCA/CN=LIP Certification Authority'"

# iber.vo.ibergrid.eu
VO_IBER_VO_IBERGRID_EU_SW_DIR=$VO_SW_DIR/iberibeu
VO_IBER_VO_IBERGRID_EU_DEFAULT_SE=$DPM_HOST
VO_IBER_VO_IBERGRID_EU_STORAGE_DIR=$CLASSIC_STORAGE_DIR/iberibeu
VO_IBER_VO_IBERGRID_EU_QUEUES="GRID_iberibeu"
VO_IBER_VO_IBERGRID_EU_VOMS_SERVERS="'vomss://voms02.ncg.ingrid.pt:8443/voms/iber.vo.ibergrid.eu?/iber.vo.ibergrid.eu'"
VO_IBER_VO_IBERGRID_EU_VOMSES="'iber.vo.ibergrid.eu voms02.ncg.ingrid.pt 40003 /C=PT/O=LIPCA/O=LIP/OU=Lisboa/CN=voms02.ncg.ingrid.pt iber.vo.ibergrid.eu'"
VO_IBER_VO_IBERGRID_EU_VOMS_CA_DN="'/C=PT/O=LIPCA/CN=LIP Certification Authority'"
